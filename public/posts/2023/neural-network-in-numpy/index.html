<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Neural Network in Numpy | iHelio</title>
<meta name="keywords" content="">
<meta name="description" content="This is to implement backpropagation algorithm in numpy which would help me to further understand how this works.
import pandas as pd 
import numpy as np
from pdb import set_trace
from sklearn import datasets
Design the network structure

Each layer contains the weights/bias and activation union

structures = [  
    {&#34;input_dim&#34;: 2, &#34;output_dim&#34;: 25, &#34;activation&#34;: &#34;relu&#34;},  
    {&#34;input_dim&#34;: 25, &#34;output_dim&#34;: 50, &#34;activation&#34;: &#34;relu&#34;},  
    {&#34;input_dim&#34;: 50, &#34;output_dim&#34;: 50, &#34;activation&#34;: &#34;relu&#34;},  
    {&#34;input_dim&#34;: 50, &#34;output_dim&#34;: 25, &#34;activation&#34;: &#34;relu&#34;},  
    {&#34;input_dim&#34;: 25, &#34;output_dim&#34;: 1, &#34;activation&#34;: &#34;sigmoid&#34;},  
]
Initiate the parameters

The weights can be random number and bias are preferred to be small postive values in order to pass the relu in the beginning.

def init_layers(structures, seed = 1105):  
    params = {}  
    for i, structure in enumerate(structures):  
        params[&#34;W_{}&#34;.format(i)] = np.random.randn(structure[&#34;input_dim&#34;], structure[&#34;output_dim&#34;])/10  
        params[&#34;b_{}&#34;.format(i)] = np.random.randint(1,10, (1, structure[&#34;output_dim&#34;]))/100  
    return params  
The forward and backword activation union

During back propagation, it is appraent we would need use the output value before activation in feed forward process. We would need to save the ouput before and after activation in each layer for back propagation later.

def relu(U):  
    U[U &lt; 0] = 0  
    return U  
  
def sigmoid(U):  
    return np.divide(1, (1&#43;np.exp(-1*U)))  
  
def relu_backward(du, U):  
    du[U &lt; 0] = 0  
    return du  

def sigmoid_backward(du, U):  
    sig = sigmoid(U) * (1 - sigmoid(U))  
    return du*sig  
So, we return two values in single_layer_feedforward function corresponding to the activated output and output which doesn&rsquo;t. The activated output will be feed as input into the next layer and the unactivated output will be used in backpropagation - the reason is we need the partial derivatives of activation union to its input.">
<meta name="author" content="Me">
<link rel="canonical" href="//localhost:1313/posts/2023/neural-network-in-numpy/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="//localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="//localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="//localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="//localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="//localhost:1313/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="//localhost:1313/posts/2023/neural-network-in-numpy/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-R4SSGN5Y2Z"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-R4SSGN5Y2Z');
        }
      </script><meta property="og:url" content="//localhost:1313/posts/2023/neural-network-in-numpy/">
  <meta property="og:site_name" content="iHelio">
  <meta property="og:title" content="Neural Network in Numpy">
  <meta property="og:description" content="This is to implement backpropagation algorithm in numpy which would help me to further understand how this works.
import pandas as pd import numpy as np from pdb import set_trace from sklearn import datasets Design the network structure Each layer contains the weights/bias and activation union structures = [ {&#34;input_dim&#34;: 2, &#34;output_dim&#34;: 25, &#34;activation&#34;: &#34;relu&#34;}, {&#34;input_dim&#34;: 25, &#34;output_dim&#34;: 50, &#34;activation&#34;: &#34;relu&#34;}, {&#34;input_dim&#34;: 50, &#34;output_dim&#34;: 50, &#34;activation&#34;: &#34;relu&#34;}, {&#34;input_dim&#34;: 50, &#34;output_dim&#34;: 25, &#34;activation&#34;: &#34;relu&#34;}, {&#34;input_dim&#34;: 25, &#34;output_dim&#34;: 1, &#34;activation&#34;: &#34;sigmoid&#34;}, ] Initiate the parameters The weights can be random number and bias are preferred to be small postive values in order to pass the relu in the beginning. def init_layers(structures, seed = 1105): params = {} for i, structure in enumerate(structures): params[&#34;W_{}&#34;.format(i)] = np.random.randn(structure[&#34;input_dim&#34;], structure[&#34;output_dim&#34;])/10 params[&#34;b_{}&#34;.format(i)] = np.random.randint(1,10, (1, structure[&#34;output_dim&#34;]))/100 return params The forward and backword activation union During back propagation, it is appraent we would need use the output value before activation in feed forward process. We would need to save the ouput before and after activation in each layer for back propagation later. def relu(U): U[U &lt; 0] = 0 return U def sigmoid(U): return np.divide(1, (1&#43;np.exp(-1*U))) def relu_backward(du, U): du[U &lt; 0] = 0 return du def sigmoid_backward(du, U): sig = sigmoid(U) * (1 - sigmoid(U)) return du*sig So, we return two values in single_layer_feedforward function corresponding to the activated output and output which doesn’t. The activated output will be feed as input into the next layer and the unactivated output will be used in backpropagation - the reason is we need the partial derivatives of activation union to its input.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-09-16T16:17:15-04:00">
    <meta property="article:modified_time" content="2023-09-16T16:17:15-04:00">
      <meta property="og:image" content="//localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="//localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name="twitter:title" content="Neural Network in Numpy">
<meta name="twitter:description" content="This is to implement backpropagation algorithm in numpy which would help me to further understand how this works.
import pandas as pd 
import numpy as np
from pdb import set_trace
from sklearn import datasets
Design the network structure

Each layer contains the weights/bias and activation union

structures = [  
    {&#34;input_dim&#34;: 2, &#34;output_dim&#34;: 25, &#34;activation&#34;: &#34;relu&#34;},  
    {&#34;input_dim&#34;: 25, &#34;output_dim&#34;: 50, &#34;activation&#34;: &#34;relu&#34;},  
    {&#34;input_dim&#34;: 50, &#34;output_dim&#34;: 50, &#34;activation&#34;: &#34;relu&#34;},  
    {&#34;input_dim&#34;: 50, &#34;output_dim&#34;: 25, &#34;activation&#34;: &#34;relu&#34;},  
    {&#34;input_dim&#34;: 25, &#34;output_dim&#34;: 1, &#34;activation&#34;: &#34;sigmoid&#34;},  
]
Initiate the parameters

The weights can be random number and bias are preferred to be small postive values in order to pass the relu in the beginning.

def init_layers(structures, seed = 1105):  
    params = {}  
    for i, structure in enumerate(structures):  
        params[&#34;W_{}&#34;.format(i)] = np.random.randn(structure[&#34;input_dim&#34;], structure[&#34;output_dim&#34;])/10  
        params[&#34;b_{}&#34;.format(i)] = np.random.randint(1,10, (1, structure[&#34;output_dim&#34;]))/100  
    return params  
The forward and backword activation union

During back propagation, it is appraent we would need use the output value before activation in feed forward process. We would need to save the ouput before and after activation in each layer for back propagation later.

def relu(U):  
    U[U &lt; 0] = 0  
    return U  
  
def sigmoid(U):  
    return np.divide(1, (1&#43;np.exp(-1*U)))  
  
def relu_backward(du, U):  
    du[U &lt; 0] = 0  
    return du  

def sigmoid_backward(du, U):  
    sig = sigmoid(U) * (1 - sigmoid(U))  
    return du*sig  
So, we return two values in single_layer_feedforward function corresponding to the activated output and output which doesn&rsquo;t. The activated output will be feed as input into the next layer and the unactivated output will be used in backpropagation - the reason is we need the partial derivatives of activation union to its input.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "//localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Neural Network in Numpy",
      "item": "//localhost:1313/posts/2023/neural-network-in-numpy/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Neural Network in Numpy",
  "name": "Neural Network in Numpy",
  "description": "This is to implement backpropagation algorithm in numpy which would help me to further understand how this works.\nimport pandas as pd import numpy as np from pdb import set_trace from sklearn import datasets Design the network structure Each layer contains the weights/bias and activation union structures = [ {\u0026#34;input_dim\u0026#34;: 2, \u0026#34;output_dim\u0026#34;: 25, \u0026#34;activation\u0026#34;: \u0026#34;relu\u0026#34;}, {\u0026#34;input_dim\u0026#34;: 25, \u0026#34;output_dim\u0026#34;: 50, \u0026#34;activation\u0026#34;: \u0026#34;relu\u0026#34;}, {\u0026#34;input_dim\u0026#34;: 50, \u0026#34;output_dim\u0026#34;: 50, \u0026#34;activation\u0026#34;: \u0026#34;relu\u0026#34;}, {\u0026#34;input_dim\u0026#34;: 50, \u0026#34;output_dim\u0026#34;: 25, \u0026#34;activation\u0026#34;: \u0026#34;relu\u0026#34;}, {\u0026#34;input_dim\u0026#34;: 25, \u0026#34;output_dim\u0026#34;: 1, \u0026#34;activation\u0026#34;: \u0026#34;sigmoid\u0026#34;}, ] Initiate the parameters The weights can be random number and bias are preferred to be small postive values in order to pass the relu in the beginning. def init_layers(structures, seed = 1105): params = {} for i, structure in enumerate(structures): params[\u0026#34;W_{}\u0026#34;.format(i)] = np.random.randn(structure[\u0026#34;input_dim\u0026#34;], structure[\u0026#34;output_dim\u0026#34;])/10 params[\u0026#34;b_{}\u0026#34;.format(i)] = np.random.randint(1,10, (1, structure[\u0026#34;output_dim\u0026#34;]))/100 return params The forward and backword activation union During back propagation, it is appraent we would need use the output value before activation in feed forward process. We would need to save the ouput before and after activation in each layer for back propagation later. def relu(U): U[U \u0026lt; 0] = 0 return U def sigmoid(U): return np.divide(1, (1+np.exp(-1*U))) def relu_backward(du, U): du[U \u0026lt; 0] = 0 return du def sigmoid_backward(du, U): sig = sigmoid(U) * (1 - sigmoid(U)) return du*sig So, we return two values in single_layer_feedforward function corresponding to the activated output and output which doesn\u0026rsquo;t. The activated output will be feed as input into the next layer and the unactivated output will be used in backpropagation - the reason is we need the partial derivatives of activation union to its input.\n",
  "keywords": [
    
  ],
  "articleBody": "This is to implement backpropagation algorithm in numpy which would help me to further understand how this works.\nimport pandas as pd import numpy as np from pdb import set_trace from sklearn import datasets Design the network structure Each layer contains the weights/bias and activation union structures = [ {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"}, {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"}, {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"}, {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"}, {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"}, ] Initiate the parameters The weights can be random number and bias are preferred to be small postive values in order to pass the relu in the beginning. def init_layers(structures, seed = 1105): params = {} for i, structure in enumerate(structures): params[\"W_{}\".format(i)] = np.random.randn(structure[\"input_dim\"], structure[\"output_dim\"])/10 params[\"b_{}\".format(i)] = np.random.randint(1,10, (1, structure[\"output_dim\"]))/100 return params The forward and backword activation union During back propagation, it is appraent we would need use the output value before activation in feed forward process. We would need to save the ouput before and after activation in each layer for back propagation later. def relu(U): U[U \u003c 0] = 0 return U def sigmoid(U): return np.divide(1, (1+np.exp(-1*U))) def relu_backward(du, U): du[U \u003c 0] = 0 return du def sigmoid_backward(du, U): sig = sigmoid(U) * (1 - sigmoid(U)) return du*sig So, we return two values in single_layer_feedforward function corresponding to the activated output and output which doesn’t. The activated output will be feed as input into the next layer and the unactivated output will be used in backpropagation - the reason is we need the partial derivatives of activation union to its input.\ndef single_layer_feedforward(A, W, b, activation_func): return activation_func(A@W + b), A@W + b Duing feed forward process, we start with features (X), and go through each layer till the final output. def feedforward(X, structures, params): U_curr = X for i, structure in enumerate(structures): # set_trace() W_curr = params[\"W_\" + str(i)] b_curr = params[\"b_\" + str(i)] params[\"U_input_\" + str(i)] = U_curr if structure[\"activation\"] == \"relu\": activation_func = relu elif structure[\"activation\"] == \"sigmoid\": activation_func = sigmoid else: print(\"no supported activation\") exit U_next, U_curr = single_layer_feedforward(U_curr, W_curr, b_curr, activation_func) params[\"U_post_activation_\" + str(i)] = U_next params[\"U_prior_activation_\" + str(i)] = U_curr U_curr = U_next return U_curr Loss function Here we used the negative log-loss as the ​ (total loss) to minimize.\ndef negativelogloss(output, y): # set_trace() return np.squeeze(-1 * sum(y * np.log10(output) + (1 - y)*np.log10(1-output)) / len(y)) def get_accuracy(y_true, y_predicted): predicted_class = y_predicted.copy() predicted_class.reshape(-1) predicted_class[predicted_class \u003e 0.5] = 1 predicted_class[predicted_class \u003c= 0.5] = 0 return accuracy_score(y_true, predicted_class) Backpropagtion process During backpropagtion, we passed the partial derivatives based on the chain rules.\nIn the single layer backward, it is obvious we will feed the previous derivatives into the layer in bottom-up order, then the derivatives will multiply the partial derivatives in the layer to generate the accumulative derivatives for next layer. During this process, we will also save the gradient of weights and bias (average gradients) in table for update later.\ndef single_layer_backward(dz, U_input, U_prior_activation, W, activation_func): m = len(dz) dz = activation_func(dz, U_prior_activation) gradient_W = (U_input.T @ dz) / m gradient_b = np.mean(dz, axis = 0) dz = dz @ W.T # set_trace() return dz, gradient_W, gradient_b During the whole backpropagtion process, we started from the partial derivatives of loss function to the y_hat (output of feed forward).\nIn each layers, we will accumulate the derivatives and calculate the gradient of W and b.\nThe accumulated derivatives will be passed to next layer.\nThe gradient of W and b in each layer will stored in grads table and we can update them later.\ngrads_table = {} def backward(output, y, structures, params): dz_prev = -(np.divide(y, output) - np.divide(1-y, 1 - output)) # set_trace() i = len(structures) - 1 while i \u003e= 0: W_curr = params[\"W_\" + str(i)] b_curr = params[\"b_\" + str(i)] U_input_curr = params[\"U_input_\" + str(i)] U_prior_activation = params[\"U_prior_activation_\" + str(i)] if structures[i][\"activation\"] == \"relu\": activation_func = relu_backward elif structures[i][\"activation\"] == \"sigmoid\": activation_func = sigmoid_backward else: print(\"Not suppported activation func\") exit # set_trace() dz_prev, gradient_W, gradient_b = single_layer_backward(dz_prev, U_input_curr, U_prior_activation, W_curr, activation_func) # set_trace() # params[\"W_\" + str(i)] = W_curr grads_table[\"gradient_W_\" + str(i)] = gradient_W grads_table[\"gradient_b_\" + str(i)] = gradient_b i -= 1 Update the parameters Go through each layer to update the parameters using grads_table def update_weights(depth, params, grads_table, lr): for i in range(depth): params[\"W_\" + str(i)] = params[\"W_\" + str(i)] - grads_table[\"gradient_W_\" + str(i)] * lr params[\"b_\" + str(i)] = params[\"b_\" + str(i)] - grads_table[\"gradient_b_\" + str(i)] * lr return params In practice import os from sklearn.datasets import make_moons from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score X, y = make_moons(n_samples = 1000, noise=0.2, random_state=100) y = y.reshape(-1, 1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) params = init_layers(structures) test = params.copy() grads_table = {} deepth = len(structures) for epoch in range(10000): output = feedforward(X_train, structures, params) # print(\"logloss: {}; accuracy: {}\".format(negativelogloss(output, y_train), get_accuracy(y_train, output))) backward(output, y_train, structures, params) params = update_weights(deepth, params, grads_table, 0.01) y_hat = feedforward(X_test, structures, params) print(\"logloss: {}; accuracy: {}\".format(negativelogloss(y_hat, y_test), get_accuracy(y_test, y_hat))) ",
  "wordCount" : "829",
  "inLanguage": "en",
  "image": "//localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished": "2023-09-16T16:17:15-04:00",
  "dateModified": "2023-09-16T16:17:15-04:00",
  "author":{
    "@type": "Person",
    "name": "Me"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "//localhost:1313/posts/2023/neural-network-in-numpy/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "iHelio",
    "logo": {
      "@type": "ImageObject",
      "url": "//localhost:1313/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="//localhost:1313/" accesskey="h" title="Home (Alt + H)">
                <img src="//localhost:1313/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="//localhost:1313/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/search/" title="Search">
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="//localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="//localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Neural Network in Numpy
    </h1>
    <div class="post-meta"><span title='2023-09-16 16:17:15 -0400 EDT'>September 16, 2023</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;829 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href="https://github.com/ydeng11/ihelio/tree/main/content/posts/2023/Neural%20Network%20in%20Numpy.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> 
  <div class="post-content"><p>This is to implement backpropagation algorithm in numpy which would help me to further understand how this works.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> 
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pdb</span> <span class="kn">import</span> <span class="n">set_trace</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
</span></span></code></pre></div><h3 id="design-the-network-structure">Design the network structure<a hidden class="anchor" aria-hidden="true" href="#design-the-network-structure">#</a></h3>
<ul>
<li>Each layer contains the weights/bias and activation union</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">structures</span> <span class="o">=</span> <span class="p">[</span>  
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="s2">&#34;input_dim&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;output_dim&#34;</span><span class="p">:</span> <span class="mi">25</span><span class="p">,</span> <span class="s2">&#34;activation&#34;</span><span class="p">:</span> <span class="s2">&#34;relu&#34;</span><span class="p">},</span>  
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="s2">&#34;input_dim&#34;</span><span class="p">:</span> <span class="mi">25</span><span class="p">,</span> <span class="s2">&#34;output_dim&#34;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s2">&#34;activation&#34;</span><span class="p">:</span> <span class="s2">&#34;relu&#34;</span><span class="p">},</span>  
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="s2">&#34;input_dim&#34;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s2">&#34;output_dim&#34;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s2">&#34;activation&#34;</span><span class="p">:</span> <span class="s2">&#34;relu&#34;</span><span class="p">},</span>  
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="s2">&#34;input_dim&#34;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s2">&#34;output_dim&#34;</span><span class="p">:</span> <span class="mi">25</span><span class="p">,</span> <span class="s2">&#34;activation&#34;</span><span class="p">:</span> <span class="s2">&#34;relu&#34;</span><span class="p">},</span>  
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="s2">&#34;input_dim&#34;</span><span class="p">:</span> <span class="mi">25</span><span class="p">,</span> <span class="s2">&#34;output_dim&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;activation&#34;</span><span class="p">:</span> <span class="s2">&#34;sigmoid&#34;</span><span class="p">},</span>  
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span></code></pre></div><h3 id="initiate-the-parameters">Initiate the parameters<a hidden class="anchor" aria-hidden="true" href="#initiate-the-parameters">#</a></h3>
<ul>
<li>The weights can be random number and bias are preferred to be small postive values in order to pass the relu in the beginning.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">init_layers</span><span class="p">(</span><span class="n">structures</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">1105</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">    <span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>  
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">structure</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">structures</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">        <span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_</span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">structure</span><span class="p">[</span><span class="s2">&#34;input_dim&#34;</span><span class="p">],</span> <span class="n">structure</span><span class="p">[</span><span class="s2">&#34;output_dim&#34;</span><span class="p">])</span><span class="o">/</span><span class="mi">10</span>  
</span></span><span class="line"><span class="cl">        <span class="n">params</span><span class="p">[</span><span class="s2">&#34;b_</span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">structure</span><span class="p">[</span><span class="s2">&#34;output_dim&#34;</span><span class="p">]))</span><span class="o">/</span><span class="mi">100</span>  
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">params</span>  
</span></span></code></pre></div><h3 id="the-forward-and-backword-activation-union">The forward and backword activation union<a hidden class="anchor" aria-hidden="true" href="#the-forward-and-backword-activation-union">#</a></h3>
<ul>
<li>During back propagation, it is appraent we would need use the output value before activation in feed forward process. We would need to save the ouput before and after activation in each layer for back propagation later.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">U</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">    <span class="n">U</span><span class="p">[</span><span class="n">U</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">U</span>  
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">U</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">U</span><span class="p">)))</span>  
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">relu_backward</span><span class="p">(</span><span class="n">du</span><span class="p">,</span> <span class="n">U</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">    <span class="n">du</span><span class="p">[</span><span class="n">U</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">du</span>  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sigmoid_backward</span><span class="p">(</span><span class="n">du</span><span class="p">,</span> <span class="n">U</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">    <span class="n">sig</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">U</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">U</span><span class="p">))</span>  
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">du</span><span class="o">*</span><span class="n">sig</span>  
</span></span></code></pre></div><p>So, we return two values in single_layer_feedforward function corresponding to the activated output and output which doesn&rsquo;t. The activated output will be feed as input into the next layer and the unactivated output will be used in backpropagation - the reason is we need the partial derivatives of activation union to its input.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">single_layer_feedforward</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">activation_func</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">activation_func</span><span class="p">(</span><span class="n">A</span><span class="nd">@W</span> <span class="o">+</span> <span class="n">b</span><span class="p">),</span> <span class="n">A</span><span class="nd">@W</span> <span class="o">+</span> <span class="n">b</span>  
</span></span></code></pre></div><ul>
<li>Duing feed forward process, we start with features (X), and go through each layer till the final output.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">feedforward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">structures</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">    <span class="n">U_curr</span> <span class="o">=</span> <span class="n">X</span>  
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">structure</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">structures</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">        <span class="c1"># set_trace()  </span>
</span></span><span class="line"><span class="cl">        <span class="n">W_curr</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>  
</span></span><span class="line"><span class="cl">        <span class="n">b_curr</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&#34;b_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>  
</span></span><span class="line"><span class="cl">        <span class="n">params</span><span class="p">[</span><span class="s2">&#34;U_input_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">U_curr</span>  
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">structure</span><span class="p">[</span><span class="s2">&#34;activation&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&#34;relu&#34;</span><span class="p">:</span>  
</span></span><span class="line"><span class="cl">            <span class="n">activation_func</span> <span class="o">=</span> <span class="n">relu</span>  
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">structure</span><span class="p">[</span><span class="s2">&#34;activation&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&#34;sigmoid&#34;</span><span class="p">:</span>  
</span></span><span class="line"><span class="cl">            <span class="n">activation_func</span> <span class="o">=</span> <span class="n">sigmoid</span>  
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>  
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;no supported activation&#34;</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">            <span class="n">exit</span>  
</span></span><span class="line"><span class="cl">        <span class="n">U_next</span><span class="p">,</span> <span class="n">U_curr</span> <span class="o">=</span> <span class="n">single_layer_feedforward</span><span class="p">(</span><span class="n">U_curr</span><span class="p">,</span> <span class="n">W_curr</span><span class="p">,</span> <span class="n">b_curr</span><span class="p">,</span> <span class="n">activation_func</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">        <span class="n">params</span><span class="p">[</span><span class="s2">&#34;U_post_activation_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">U_next</span>  
</span></span><span class="line"><span class="cl">        <span class="n">params</span><span class="p">[</span><span class="s2">&#34;U_prior_activation_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">U_curr</span>  
</span></span><span class="line"><span class="cl">        <span class="n">U_curr</span> <span class="o">=</span> <span class="n">U_next</span>  
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">U_curr</span>  
</span></span></code></pre></div><h3 id="loss-function">Loss function<a hidden class="anchor" aria-hidden="true" href="#loss-function">#</a></h3>
<p>Here we used the negative log-loss as the ​ (total loss) to minimize.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">negativelogloss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">    <span class="c1"># set_trace()  </span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">output</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_accuracy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">    <span class="n">predicted_class</span> <span class="o">=</span> <span class="n">y_predicted</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>  
</span></span><span class="line"><span class="cl">    <span class="n">predicted_class</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">    <span class="n">predicted_class</span><span class="p">[</span><span class="n">predicted_class</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  
</span></span><span class="line"><span class="cl">    <span class="n">predicted_class</span><span class="p">[</span><span class="n">predicted_class</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">predicted_class</span><span class="p">)</span>  
</span></span></code></pre></div><h3 id="backpropagtion-process">Backpropagtion process<a hidden class="anchor" aria-hidden="true" href="#backpropagtion-process">#</a></h3>
<ul>
<li>
<p>During backpropagtion, we passed the partial derivatives based on the chain rules.</p>
</li>
<li>
<p>In the single layer backward, it is obvious we will feed the previous derivatives into the layer in bottom-up order, then the derivatives will multiply the partial derivatives in the layer to generate the accumulative derivatives for next layer. During this process, we will also save the gradient of weights and bias (average gradients) in table for update later.</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">single_layer_backward</span><span class="p">(</span><span class="n">dz</span><span class="p">,</span> <span class="n">U_input</span><span class="p">,</span> <span class="n">U_prior_activation</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">activation_func</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dz</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dz</span> <span class="o">=</span> <span class="n">activation_func</span><span class="p">(</span><span class="n">dz</span><span class="p">,</span> <span class="n">U_prior_activation</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">gradient_W</span> <span class="o">=</span> <span class="p">(</span><span class="n">U_input</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dz</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">    <span class="n">gradient_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dz</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dz</span> <span class="o">=</span> <span class="n">dz</span> <span class="o">@</span> <span class="n">W</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl"><span class="c1">#     set_trace()</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dz</span><span class="p">,</span> <span class="n">gradient_W</span><span class="p">,</span> <span class="n">gradient_b</span>
</span></span></code></pre></div><ul>
<li>
<p>During the whole backpropagtion process, we started from the partial derivatives of loss function to the y_hat (output of feed forward).</p>
</li>
<li>
<p>In each layers, we will accumulate the derivatives and calculate the gradient of W and b.</p>
</li>
<li>
<p>The accumulated derivatives will be passed to next layer.</p>
</li>
<li>
<p>The gradient of W and b in each layer will stored in grads table and we can update them later.</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">grads_table</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">structures</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">dz_prev</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># set_trace()</span>
</span></span><span class="line"><span class="cl">    <span class="n">i</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">structures</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">W_curr</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="n">b_curr</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&#34;b_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="n">U_input_curr</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&#34;U_input_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="n">U_prior_activation</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&#34;U_prior_activation_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">structures</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&#34;activation&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&#34;relu&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">activation_func</span> <span class="o">=</span> <span class="n">relu_backward</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">structures</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&#34;activation&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&#34;sigmoid&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">activation_func</span> <span class="o">=</span> <span class="n">sigmoid_backward</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Not suppported activation func&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">exit</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         set_trace()</span>
</span></span><span class="line"><span class="cl">        <span class="n">dz_prev</span><span class="p">,</span> <span class="n">gradient_W</span><span class="p">,</span> <span class="n">gradient_b</span> <span class="o">=</span> <span class="n">single_layer_backward</span><span class="p">(</span><span class="n">dz_prev</span><span class="p">,</span> <span class="n">U_input_curr</span><span class="p">,</span> <span class="n">U_prior_activation</span><span class="p">,</span> <span class="n">W_curr</span><span class="p">,</span> <span class="n">activation_func</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         set_trace()</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         params[&#34;W_&#34; + str(i)] = W_curr</span>
</span></span><span class="line"><span class="cl">        <span class="n">grads_table</span><span class="p">[</span><span class="s2">&#34;gradient_W_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">gradient_W</span>
</span></span><span class="line"><span class="cl">        <span class="n">grads_table</span><span class="p">[</span><span class="s2">&#34;gradient_b_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">gradient_b</span>
</span></span><span class="line"><span class="cl">        <span class="n">i</span> <span class="o">-=</span> <span class="mi">1</span>
</span></span></code></pre></div><h3 id="update-the-parameters">Update the parameters<a hidden class="anchor" aria-hidden="true" href="#update-the-parameters">#</a></h3>
<ul>
<li>Go through each layer to update the parameters using grads_table</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads_table</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">-</span> <span class="n">grads_table</span><span class="p">[</span><span class="s2">&#34;gradient_W_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">*</span> <span class="n">lr</span>
</span></span><span class="line"><span class="cl">        <span class="n">params</span><span class="p">[</span><span class="s2">&#34;b_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&#34;b_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">-</span> <span class="n">grads_table</span><span class="p">[</span><span class="s2">&#34;gradient_b_&#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">*</span> <span class="n">lr</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">params</span>
</span></span></code></pre></div><h3 id="in-practice">In practice<a hidden class="anchor" aria-hidden="true" href="#in-practice">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">params</span> <span class="o">=</span> <span class="n">init_layers</span><span class="p">(</span><span class="n">structures</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">grads_table</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="n">deepth</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">structures</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="n">feedforward</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">structures</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#     print(&#34;logloss: {}; accuracy: {}&#34;.format(negativelogloss(output, y_train), get_accuracy(y_train, output)))</span>
</span></span><span class="line"><span class="cl">    <span class="n">backward</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">structures</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">params</span> <span class="o">=</span> <span class="n">update_weights</span><span class="p">(</span><span class="n">deepth</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads_table</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="n">y_hat</span> <span class="o">=</span> <span class="n">feedforward</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">structures</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;logloss: </span><span class="si">{}</span><span class="s2">; accuracy: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">negativelogloss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> <span class="n">get_accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)))</span>
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="//localhost:1313/posts/2023/mastering-the-art-of-car-dealership-negotiations/">
    <span class="title">« Prev</span>
    <br>
    <span>Mastering the Art of Car Dealership Negotiations</span>
  </a>
  <a class="next" href="//localhost:1313/posts/2023/sneak-peek-at-the-asynchronous-java/">
    <span class="title">Next »</span>
    <br>
    <span>Sneak peek at the asynchronous Java</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network in Numpy on x"
            href="https://x.com/intent/tweet/?text=Neural%20Network%20in%20Numpy&amp;url=%2f%2flocalhost%3a1313%2fposts%2f2023%2fneural-network-in-numpy%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network in Numpy on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2f%2flocalhost%3a1313%2fposts%2f2023%2fneural-network-in-numpy%2f&amp;title=Neural%20Network%20in%20Numpy&amp;summary=Neural%20Network%20in%20Numpy&amp;source=%2f%2flocalhost%3a1313%2fposts%2f2023%2fneural-network-in-numpy%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network in Numpy on reddit"
            href="https://reddit.com/submit?url=%2f%2flocalhost%3a1313%2fposts%2f2023%2fneural-network-in-numpy%2f&title=Neural%20Network%20in%20Numpy">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network in Numpy on facebook"
            href="https://facebook.com/sharer/sharer.php?u=%2f%2flocalhost%3a1313%2fposts%2f2023%2fneural-network-in-numpy%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network in Numpy on whatsapp"
            href="https://api.whatsapp.com/send?text=Neural%20Network%20in%20Numpy%20-%20%2f%2flocalhost%3a1313%2fposts%2f2023%2fneural-network-in-numpy%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network in Numpy on telegram"
            href="https://telegram.me/share/url?text=Neural%20Network%20in%20Numpy&amp;url=%2f%2flocalhost%3a1313%2fposts%2f2023%2fneural-network-in-numpy%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network in Numpy on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Neural%20Network%20in%20Numpy&u=%2f%2flocalhost%3a1313%2fposts%2f2023%2fneural-network-in-numpy%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
<div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "ihelio-today" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="//localhost:1313/">iHelio</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
